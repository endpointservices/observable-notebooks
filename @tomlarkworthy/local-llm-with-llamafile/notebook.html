<!doctype html>
<notebook theme="air">
  <title>Local LLM with llamafile</title>
  <script id="0" type="text/markdown">
    # Local LLM with [llamafile](https://github.com/Mozilla-Ocho/llamafile)

    Run a LLM + webserver locally, its very easy with [llamafiles](https://github.com/Mozilla-Ocho/llamafile)

    ```
    chmod +x mistral-7b-instruct-v0.2.Q5_K_M.llamafile
    ./mistral-7b-instruct-v0.2-Q5_K_M-server.llamafile
    ```

    I had problems running the Q5 weights on an M1 Pro 16GB. But the Q4 worked and I extracted the Q4 weights and ran the latest server on those (see [llamafile/issues/24](https://github.com/Mozilla-Ocho/llamafile/issues/24))

    ```
    ./llamafile-0.5 --server -m mistral-7b-instruct-v0.1.Q4_K_M.gguf
    ```

    Running that servers a chat interface on [localhost:8080](https://localhost:8080), which can return return good results at about the same speed as OpenAI, on a Mac M1:

    <details>
      <summary>screenshot</summary>
      ![image.png](${await FileAttachment("image.png").url()})
    </details>

  </script>
  <script id="36" type="application/vnd.observable.javascript">
    viewof max_tokens = Inputs.range([0, 4096], {
      label: "max_tokens",
      value: 4096
    })
  </script>
  <script id="34" type="application/vnd.observable.javascript">
    settings = ({
      temperature: 0.7,
      max_tokens,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0
    })
  </script>
  <script id="19" type="text/markdown">
    ## Call the API

    In addition to the web app, the server also serves much of OpenAI's API (function calling is a notable ommision)

    <details>
      <summary>example shreenshot</summary>
      ![image@2.png](${await FileAttachment("image@2.png").url()})
    </details>
  </script>
  <script id="31" type="application/vnd.observable.javascript">
    viewof input = {
      return Inputs.textarea({
        placeholder: "write your prompt here",
        rows: 100,
        submit: true
      });
    }
  </script>
  <script id="70" type="application/vnd.observable.javascript">
    md`${response.choices[0].message.content}`
  </script>
  <script id="26" type="application/vnd.observable.javascript">
    response = {
      const response = await fetch("http://localhost:8080/v1/chat/completions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [
            {
              role: "user",
              content: input
            }
          ],
          ...settings
        })
      });

      if (response.status !== 200)
        throw new Error(`${response.status}: ${await response.text()}`);

      return response.json();
    }
  </script>
  <script id="85" type="text/markdown">
    ## Streaming responses

    Llamafile server also supports streaming
  </script>
  <script id="88" type="application/vnd.observable.javascript">
    viewof streaming_input = {
      return Inputs.textarea({
        placeholder: "write your prompt here",
        rows: 100,
        submit: true
      });
    }
  </script>
  <script id="159" type="application/vnd.observable.javascript">
    md`${streaming_response.content || ""}`
  </script>
  <script id="93" type="application/vnd.observable.javascript">
    streaming_events = Generators.observe((notify) => {
      let events = [];
      fetchEventSource("http://localhost:8080/v1/chat/completions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [
            {
              role: "user",
              content: streaming_input
            }
          ],
          stream: true,
          ...settings
        }),
        onmessage: (ev) => {
          //data.choices.content = ev.data.choices;
          events.push(JSON.parse(ev.data));
          notify(events);
        }
      });
    })
  </script>
  <script id="144" type="application/vnd.observable.javascript">
    JSON.stringify(streaming_events)
  </script>
  <script id="139" type="application/vnd.observable.javascript">
    streaming_response = streaming_events.reduce((data, event) => {
      const choice = event.choices[0];

      // Check if there is a 'delta' object
      if (choice.delta) {
        // Apply delta updates
        for (const key in choice.delta) {
          data[key] = (data[key] || "") + choice.delta[key];
        }
      } else {
        // If no delta, assume setting the entire data object
        data = { ...data, ...choice };
      }

      return data;
    }, {})
  </script>
  <script id="102" type="application/vnd.observable.javascript">
    fetchEventSource = (
      await import(
        "https://cdn.skypack.dev/@microsoft/fetch-event-source@2.0.1?min"
      )
    ).fetchEventSource
  </script>
</notebook>
