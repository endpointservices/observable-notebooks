<!doctype html>
<notebook theme="air">
  <title>Soundcloud Reactive Audio Visualizer Music Video Generator</title>
  <script id="0" type="application/vnd.observable.javascript">
    md`# Soundcloud Reactive Audio Visualizer Music Video Generator

    This is a tool for generating an acceptable music video from any Soundcloud track. Scroll down to find customize the audio input URL and put whatever you like. Press record to generate a downloadable _webm_ for sharing. Or just chill out by going fullscreen. More options are scattered throughout the page (e.g. video resolution or customization of visuals). I hope it might help a few artists with video production.

    This app was developed to help me learn a few things before building a full audio visual mixing demoscene system. In particular, you can click under the hood, fork the code and create your own experiences. Its licensed as ISC so you can do what you like with the code and output, no need to credit me, though I would be honoured.

    This demos the following technologies
      1. Using the Web Audio API to extract a waveform to drive reactive visuals
      1. Using Three.js effects composer to create multi layed visuals with post processing effects
      1. Importing audio from external sources (Soundcloud)
      1. Setting up an high performance Web Audio API graph.
      1. Extracting a combined audio and visual into a single video file.
      1. Using the wake lock API to prevent the host sleeping when fullscreened.
      1. Inserting HTML into a Three.js scene.

    These are foundational techniques for doing even cooler things in the browser! Stay tuned! Use the comment feature to get in touch.
    `

  </script>
  <script id="79" type="application/vnd.observable.javascript">
    {
      var controls = null;
      const rewind = () => {
        stop()
        source.mediaElement.currentTime = 0;
      }

      const play = () => {
        console.log("play")
        context.resume()
        source.connect(analyser);
        analyser.connect(context.destination);
        source.mediaElement.play();
      }
      const stop = () => {
        console.log("stop")
        source.mediaElement.pause();
      }
      const fullscreen = async (event) => {
        controls.parentElement.nextElementSibling.requestFullscreen();
        await navigator.wakeLock.request('screen');
      }
      const record = async (event) => {
        document.getElementById("downloadlink").innerHTML = ""
        var recordedChunks = [];
        return new Promise(function (res, rej) {
          const canvas = document.getElementById("visualizer");
          const video = canvas.captureStream();
          const audio = context.createMediaStreamDestination();
          analyser.connect(audio)
          const stream = new MediaStream([video.getVideoTracks()[0], audio.stream.getAudioTracks()[0]])
          const mediaRecorder = new MediaRecorder(stream, {
            mimeType: "video/webm; codecs=vp9"
          });

          mediaRecorder.start();
          play()
          function stopRecorder() {
            mediaRecorder.stop();
            source.mediaElement.removeEventListener("pause", stopRecorder, true);
            source.mediaElement.removeEventListener('ended', stopRecorder, true);
          }
          source.mediaElement.addEventListener("pause", stopRecorder, true);
          source.mediaElement.addEventListener('ended', stopRecorder, true);

          mediaRecorder.ondataavailable = function (event) {
            console.log("Chunk recorded", event)
            recordedChunks.push(event.data);
          }

          mediaRecorder.onstop = function (event) {
            console.log("stopped")
            var blob = new Blob(recordedChunks, {
              type: "video/webm"
            });
            var url = URL.createObjectURL(blob);
            const downloadLink = document.getElementById("downloadlink")
            downloadLink.href = url
            downloadLink.innerHTML = "download"
          }
        })
      }
      controls = html`
        <button onclick=${rewind}>rewind</button>
        <button onclick=${play}>play</button>
        <button onclick=${stop}>stop</button>
        <button onclick=${fullscreen}>fullscreen</button>
        <button onclick=${record}>record</button>
        <a id="downloadlink" download></a>
      `
      return controls;
    }

  </script>
  <script id="183" type="application/vnd.observable.javascript">
    {
      const width = resolution.width;
      const height = resolution.height;

      var camera = new THREE.OrthographicCamera(width / -2, width / 2,
                                                height / 2, height / -2,
                                                0, height / -2);


      const renderer = new THREE.WebGLRenderer();
      const renderTarget = new THREE.WebGLRenderTarget( width, height, {
        depthBuffer: false
      });

      renderer.setClearColor(new THREE.Color(0x000000, 1.0));
      renderer.domElement.id = "visualizer"

      invalidation.then(() => renderer.dispose());
      renderer.setSize(width, height);
      const composer = new EffectComposer( renderer, renderTarget);

      const scene = new THREE.Scene();
      scene.background = new THREE.Color(0x000000);
      scene.add(buffers.line);



      const renderScene = new RenderPass( scene, camera );
      const bloomPass = new UnrealBloomPass( new THREE.Vector2( width, height), 1.5, 0.4, 0.85 );
      bloomPass.threshold = 0;
    	bloomPass.strength = 7;
    	bloomPass.radius = 1;

    	composer.addPass( renderScene );
    	composer.addPass( bloomPass );


      { // Credits
        const texture = new THREE.TextureLoader().load(title.img);
        texture.needsUpdate = true;
        var material = new THREE.MeshBasicMaterial({
            map: texture,
            side: THREE.DoubleSide,
            transparent: true
        });
        const geometry = new THREE.PlaneBufferGeometry(title.width, title.height);
        const mesh = new THREE.Mesh( geometry, material );
        mesh.position.set(
          width/2.0 - title.width/2.0,
          - height/2.0 + title.height / 2.0,
          0)

        const titleScene = new THREE.Scene();
        titleScene.add(mesh);
        const renderTitle = new RenderPass( titleScene, camera);
        renderTitle.clear = false;
    	  composer.addPass( renderTitle );
        const copyPass = new ShaderPass( CopyShader );
        copyPass.renderToScreen = true;
    	  composer.addPass( copyPass );
      }


      const t_start = performance.now();

      while (true) {
        mutable t = performance.now() - t_start
        composer.render();
        yield renderer.domElement;
      }
    }
  </script>
  <script id="1140" type="application/vnd.observable.javascript" pinned="">
    resolution = JSON.parse(resolution_str)
  </script>
  <script id="1115" type="application/vnd.observable.javascript">
    viewof resolution_str = select([
        {label: `screen (${width}, ${height})`, value: JSON.stringify({width:width, height:height})},
        {label: "1080p (1920x1080)",  value: JSON.stringify({width:1920, height:1080})},
      ]);
  </script>
  <script id="480" type="application/vnd.observable.javascript">
    md`Create a global Audio Context`
  </script>
  <script id="152" type="application/vnd.observable.javascript">
    context = new AudioContext()
  </script>
  <script id="491" type="application/vnd.observable.javascript">
    md`Create an Media Element Source. We use the Sound Cloud API to get a stream URL based on the URL provided,
    then build a media element around it. Choose your own song by pasting the link in the box, this will update the URL, so you can share this page with your favourite song already preloaded`
  </script>
  <script id="277" type="application/vnd.observable.javascript">
    viewof source = render(({ useSetter }) => {
      const [searchInput, setSearchInput] = useState(
        decodeURIComponent(
          (
            location.hash ||
            "#https://soundcloud.com/dexcelldnb/dexcell-something-from-nothing-vip"
          ).substring(1)
        )
      );
      const [search, setSearch] = useState(searchInput);
      const [track, setTrack] = useState(null);
      const [media, setMedia] = useState(null);

      useEffect(() => {
        SC.resolve(search)
          .then((track) => setTrack(track))
          .catch((e) => setTrack(null));
      }, [search]);

      useEffect(() => {
        if (track === null) {
          setMedia(null);
        } else {
          const audio = new Audio();
          audio.crossOrigin = "anonymous";
          audio.src = track.stream_url + "?client_id=" + SC.client_id;
          setMedia(context.createMediaElementSource(audio));
        }
      }, [track]);

      useEffect(() => {
        // change the top url
        if (media !== null) document.getElementById("audioviz").click();
      }, [media]);

      const view = media || {};
      view.search = search;
      view.track = track;
      useSetter(view);

      return jsx`
        <center>
        <input
          type="text"
          placeholder="paste Sound Cloud URL here"
          value=${searchInput}
          onChange=${(e) => setSearchInput(e.target.value)}
          onKeyDown=${(e) => e.key === "Enter" && setSearch(searchInput)}
        />
        ${
          track &&
          jsx`
          <h4><a href=${track.user.permalink_url} target="_blank">${
            track.user.username
          }</a></h4>
          <a href=${track.permalink_url} target="_blank">
            <h3>${track.title}</h3>
            <img width=${width / 2}src=${track.artwork_url}/>
          </a>

          <p><a id="audioviz" href=${
            document.baseURI + "#" + encodeURIComponent(search)
          }>audioviz</a></p>
        `
        }

        </center>
      `;
    })
  </script>
  <script id="1099" type="application/vnd.observable.javascript">
    md`Its simplest to use normal HTML for the subtitles`
  </script>
  <script id="800" type="application/vnd.observable.javascript">
    titleHtml = {
      const view = html`
        <style> 
          #titleHtml {
            text-align: right;
            z-index: 100;
            display:inline-block;
            color: lightgray;
            padding-right: 20px;
            padding-bottom: 10px;
          }
        </style>
        <div id="titleHtml">
          <h2>${source.track ? source.track.user.username: "artist"}</h2>
          <h1>${source.track ? source.track.title: "title"}</h1>
        </div>`
      return view
    } 
  </script>
  <script id="1105" type="application/vnd.observable.javascript">
    md`We use a technology called dom-to-image to turn normal HTML into an image we can insert into the Three.js scene as a texture`
  </script>
  <script id="813" type="application/vnd.observable.javascript">
    title = {
      titleHtml
      const title = document.getElementById('titleHtml')
      return {
        element: title,
        width: title.offsetWidth ,
        height: title.offsetHeight,
        img: await dom2img.toPng(title)
      }
    }
  </script>
  <script id="791" type="application/vnd.observable.javascript">
    dom2img = require('https://bundle.run/dom-to-image@2.6.0')
  </script>
  <script id="485" type="application/vnd.observable.javascript">
    md`Create an analyser, which is able to capture waveforms as they are processed on the audio graph.`
  </script>
  <script id="154" type="application/vnd.observable.javascript">
    analyser = context.createAnalyser()
  </script>
  <script id="365" type="application/vnd.observable.javascript">
    source_debug = source
  </script>
  <script id="501" type="application/vnd.observable.javascript">
    md`Buffers are THREE.js geometry holders which are static. We will be fiddling with the geometry based on the sound waveform, but the scene graph remains static throughout`
  </script>
  <script id="177" type="application/vnd.observable.javascript">
    buffers = {
      // Buffers hold the static geometry which we manipulate elsewhere.
      const buffers = {
        positions: new THREE.Float32BufferAttribute( (MAX_SEGMENTS) * 3, 3 ),
        colors: new THREE.Float32BufferAttribute( (MAX_SEGMENTS) * 3, 3 ),
        lineGeometry: new THREE.BufferGeometry(),
        lineMaterial: new THREE.LineBasicMaterial( { color: 0xffffff, vertexColors: true} ),
        dataArray: new Uint8Array(analyser.frequencyBinCount),
      }
      buffers.positions.set(init.positions)
      buffers.colors.set(init.colors)
      buffers.lineGeometry.setAttribute('position', buffers.positions)
      buffers.lineGeometry.setAttribute('color', buffers.colors)
      buffers.line = new THREE.Line( buffers.lineGeometry, buffers.lineMaterial )  
      buffers.lineGeometry.setDrawRange(0, MAX_SEGMENTS);
      return buffers;
    };
  </script>
  <script id="508" type="application/vnd.observable.javascript">
    md`Our visualizer animation loop bumps the time variable _t_, causing dependant code to run`
  </script>
  <script id="191" type="application/vnd.observable.javascript">
    mutable t = 0.0
  </script>
  <script id="512" type="application/vnd.observable.javascript">
    md`Our core drawing code. When _t_ is bumped by rendering, the draw code runs. We use the data from the Audio Analyser to updating the line geometry`
  </script>
  <script id="175" type="application/vnd.observable.javascript" pinned="">
    draw = {
      t; // force this cell to update when time increments
      if (source.mediaElement.paused) return;
      const width = resolution.width;
      const height = resolution.height;

      const points = [];
      const colors = [];
      const point = new THREE.Vector3();
      const color = new THREE.Color();

      // Rotating colors
      color.setRGB(
        Math.cos(t * 0.00011) * 0.5 + 1,
        Math.cos(t * 0.002) * 0.5 + 1,
        Math.cos(t * 0.003) * 0.5 + 1
      );

      // Positions driven by audio analyser
      analyser.getByteTimeDomainData(buffers.dataArray);
      for (let i = 0; i < buffers.dataArray.length; i++) {
        var mod = 1; // Math.sin(i * Math.PI / buffers.dataArray.length)
        var y = ((buffers.dataArray[i] * height) / 256.0 - height / 2.0) * mod;
        var x = -width / 2 + (i * width) / buffers.dataArray.length;
        points.push(x, y, 0);
        colors.push(color.r, color.g, color.b);
      }
      // loop wave form round to enter the view from the left again
      points.push(width * 2, 0, 0);
      points.push(width * 2, height * 2, 0);
      points.push(-width * 2, height * 2, 0);
      points.push(-width * 2, 0, 0);

      // The main stratergy is to shift all the buffers down, and append new data points to the end
      buffers.positions.set(buffers.positions.array.slice(points.length));
      buffers.positions.set(points, buffers.positions.count * 3 - points.length);
      buffers.positions.needsUpdate = true;

      // Decay the colors by a factor so the old lines fade out
      buffers.colors.set(
        buffers.colors.array.slice(colors.length).map((x) => x * 0.5),
        0
      );
      buffers.colors.set(colors, buffers.colors.count * 3 - colors.length);
      buffers.colors.needsUpdate = true;
    }
  </script>
  <script id="185" type="application/vnd.observable.javascript">
    height = window.screen.height
  </script>
  <script id="187" type="application/vnd.observable.javascript">
    MAX_SEGMENTS = 1024*4;
  </script>
  <script id="10" type="application/vnd.observable.javascript">
    SC = {
      const sc = await require('soundcloud');
      sc.client_id = "95f22ed54a5c297b1c41f72d713623ef"
      sc.initialize({
        client_id: sc.client_id,
        redirect_uri: 'https://example.com/callback'
      });
      return sc
    }
  </script>
  <script id="181" type="application/vnd.observable.javascript">
    THREE_VERSION = "0.112.1"
  </script>
  <script id="179" type="application/vnd.observable.javascript">
    THREE = require(`three@${THREE_VERSION}/build/three.min.js`);
  </script>
  <script id="189" type="application/vnd.observable.javascript">
    EffectComposer = (await import(`https://unpkg.com/three@${THREE_VERSION}/examples/jsm/postprocessing/EffectComposer.js?module`)).EffectComposer
  </script>
  <script id="195" type="application/vnd.observable.javascript">
    UnrealBloomPass = (await import(`https://unpkg.com/three@${THREE_VERSION}/examples/jsm/postprocessing/UnrealBloomPass.js?module`)).UnrealBloomPass
  </script>
  <script id="193" type="application/vnd.observable.javascript">
    RenderPass = (await import(`https://unpkg.com/three@${THREE_VERSION}/examples/jsm/postprocessing/RenderPass.js?module`)).RenderPass
  </script>
  <script id="966" type="application/vnd.observable.javascript">
    ShaderPass = (await import(`https://unpkg.com/three@${THREE_VERSION}/examples/jsm/postprocessing/ShaderPass.js?module`)).ShaderPass
  </script>
  <script id="973" type="application/vnd.observable.javascript">
    CopyShader = (await import(`https://unpkg.com/three@${THREE_VERSION}/examples/jsm/shaders/CopyShader.js?module`)).CopyShader
  </script>
  <script id="104" type="application/vnd.observable.javascript">
    import {html} from "@observablehq/htl"
  </script>
  <script id="1117" type="application/vnd.observable.javascript">
    import {select} from "@jashkenas/inputs"
  </script>
  <script id="199" type="application/vnd.observable.javascript">
    import {checkbox} from "@jashkenas/inputs"
  </script>
  <script id="273" type="application/vnd.observable.javascript">
    import {render, component, jsx, memo, forwardRef, React, ReactDOM, createElement, Children, createRef, createContext, lazy, Fragment, StrictMode, Suspense, cloneElement, useCallback, useContext, useEffect, useImperativeHandle, useLayoutEffect, useMemo, useReducer, useRef, useState} from '@j-f1/react-16'
  </script>
  <script id="524" type="application/vnd.observable.javascript">
    init = ({
      positions: JSON.parse(await FileAttachment("positions.json").text()),
      colors: JSON.parse(await FileAttachment("colors.json").text())
    })
  </script>
  <script id="1179" type="application/vnd.observable.javascript">
    import { footer } from "@tomlarkworthy/footer"
  </script>
  <script id="1182" type="application/vnd.observable.javascript">
    footer
  </script>
</notebook>
