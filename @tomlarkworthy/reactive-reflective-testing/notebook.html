<!doctype html>
<notebook theme="air">
  <title>Reactive Reflective Testing in Lopebook</title>
  <script id="0" type="text/markdown">
    # Reactive Reflective Testing in Lopebook
  </script>
  <script id="11" type="text/markdown">
    Lobebooks are single files web pages you can open directly without a webserver but that offer a fully editable literate programming experience.

    Lopecode notebooks don't make a distinction between the application notebook and the development tooling used modify the notebook. Development tooling is implemented as notebooks that interact with a single shared runtime. So a lopebook is inherently a coordinated multi-notebook experience.

    ![image@1.png](${await FileAttachment("image@1.png").url()})

    _All_ notebooks have the same reactive programming model. This open's up a few novel consequences for development, that I think makes reactive development *better* than current tooling. In this post I will explain the impact on unit testing specifically.

    1. Reactive test-driven development enables faster development loops by rerunning only the tests impacted by a change.
    2. Reflection enables automatic test discovery over the entire runtime.
    3. Userspace implementation allows reusing userspace code as material for testing, greatly removing the need to think of test cases.

    Put together its like delivering an always-on low latency CI that efficiently runs over all current and future code forever. That's not the only benefit of reactive notebooks bring, but its what we concentrate on here.
  </script>
  <script id="19" type="text/markdown">
    ## Reactive Unit Testing

    Unit tests are used to test a small piece of code for expected behaviour. They are widely used to protect against accidental regression. They should be fast to run and be part of the core development loop. Industry unit testing tools often offer a _watch mode_, which automatically reruns the unit testing suite on an source code change. 

    _Watch mode_ has been found to beneficial to development velocity by shortening the time between a coding phase and a verification phase which are core activities during development. Reactive unit testing is like _watch mode_ on steroids!




  </script>
  <script id="58" type="text/markdown">
    ### How the reactive runtime works
  </script>
  <script id="60" type="text/markdown">
    Before we can explain reactive testing, let us understand how reactivity works in general. A reactive program is broken down into `cells`. The cell's source code is scanned for external identifies (see the [toolchain](https://observablehq.com/@tomlarkworthy/observablejs-toolchain)), which are considered dependancies. The inter-cell dependancies form a dependancy graph. The runtime schedules recomputation of individual cells if their values inputs are recomputed, or if their source code is changed. 

  </script>
  <script id="64" type="text/markdown">
    This implies is cells are only run when they need to run. Small changes to the graph cascade only to only those cells are affected. Existing values elsewhere remain unchanged and available. This is similar to hot-code reload, or partial compilation. It is less traumatic than rerunning a tradition program, where the entire program memory is reset. It allows you to change the program on the fly, which simultaneously using the program. It avoids you having to "go back to the beginning" when iterating.
  </script>
  <script id="79" type="text/markdown">
    ### Faster watch mode
  </script>
  <script id="83" type="text/markdown">
    Because of the explicit dependancy graph, time to calculate unit tests is improved. When source code changes, only unit tests impacted in the wake are recomputed. This permits larger test suites for the same computational resources. This is all fully automatic and something the programmer does not need to think about, its a consequence of how a reactive runtime works. 
  </script>
  <script id="93" type="text/markdown">
    ### Examples

    In these examples we reactively link the test to an input parameter to test a trigonometric identity. Changing the input recomputes the affected computations only. So only a single test is rerun. Note how only one test flickers as the test parameters change.
  </script>
  <script id="137" type="text/markdown">
    ${tex`0 = sin(a) - cos(a - 0.5\pi)`} ?
  </script>
  <script id="95" type="application/vnd.observable.javascript">
    viewof a = Inputs.range([-Math.PI * 2, Math.PI * 2], {
      label: "a",
      step: 0.1
    })
  </script>
  <script id="123" type="application/vnd.observable.javascript">
    sin_a = Math.sin(a)
  </script>
  <script id="130" type="application/vnd.observable.javascript">
    cos_a_minus_half_pi = Math.cos(a - 0.5 * Math.PI)
  </script>
  <script id="128" type="application/vnd.observable.javascript">
    test_sin_is_cos = {
      if (Math.abs(sin_a - cos_a_minus_half_pi) > 0.0001) throw Error();
    }
  </script>
  <script id="154" type="text/markdown">
    ${tex`1 = sin^2(b) + cos^2(b)`} ?
  </script>
  <script id="160" type="application/vnd.observable.javascript">
    viewof b = Inputs.range([-Math.PI * 2, Math.PI * 2], {
      label: "b",
      step: 0.1
    })
  </script>
  <script id="163" type="application/vnd.observable.javascript">
    sin_sq_b = Math.sin(b) * Math.sin(b)
  </script>
  <script id="166" type="application/vnd.observable.javascript">
    cos_sq_b = Math.cos(b) * Math.cos(b)
  </script>
  <script id="170" type="application/vnd.observable.javascript">
    test_sin_sq_plus_cos_sq = {
      if (Math.abs(1 - sin_sq_b - cos_sq_b) > 0.0001) throw Error();
    }
  </script>
  <script id="204" type="application/vnd.observable.javascript">
    tests({
      filter: (test) => test.name.includes("_sin_")
    })
  </script>
  <script id="182" type="text/markdown">
    ## Reflective Test Discovery
  </script>
  <script id="184" type="text/markdown">
    Another benefit of a centralized reactive runtime is notebooks can programatically query for cells, and programatically add, remove and update cells. This essential to support building the internal editor because the editor must, of course, support mutating the source. But runtime reflection and programatic editing is useful for all sorts of development tooling, including testing. 
  </script>
  <script id="197" type="application/vnd.observable.javascript">
    import { tests } from "@tomlarkworthy/tests"
  </script>
  <script id="189" type="text/markdown">
    For instance, the [tests](https://observablehq.com/@tomlarkworthy/tester) notebook searches for cells prefixed with `test_` and [`observe`s](https://observablehq.com/@tomlarkworthy/runtime-sdk#cell-159) them. Observation streams all state changes of a cell to a handler function. For tests we consider the cell to have passed if it does not reject (throw an Error).
  </script>
  <script id="216" type="text/markdown">
    This allows the tests notebook to provide a summary of all tests within the notebook network, without any direct coupling.
  </script>
  <script id="224" type="text/markdown">
    One detail of the Runtime is that only rendered cells, or their dependancies, are actually run. So the `tests` UI shows all tests, but only the subset that are reachable are computed.
  </script>
  <script id="229" type="text/markdown">
    Named cells can be uniquely identified by their name and the name of the notebook they reside in (sometimes called a module). The test UI is thus able to provide hyperlinks to the code, enabling fast navigation to the test. In lopecode this will open the enclosing notebook, and thus rendering it, and thus making unreachable cells now reachable.
  </script>
  <script id="241" type="text/markdown">
    Programatic runtime access is useful for centralising test suite status. It means tests can be written where they contextually make sense, _i.e._ near the implementation, but with the status reported to a central location which can be easily checked. Its not the only use-case for reflective meta-programming though. We can use self-querying for all kinds of development aids, architecture summaries, linting, LLM context building _etc._. Notebook environments are a unique substrate where both the source code, and the program values are available at the same time and in sync.
  </script>
  <script id="227" type="text/markdown">
    ## Whole notebook invariant testing
  </script>
  <script id="256" type="text/markdown">
    Unit tests usually tests a small piece of code, focusing on a specific feature in a specific configuration. Invariant testing on-the-other hand checks that some property holds over all configurations.

    Showing that an invariant holds over an infinite configuration space requires logical reasoning e.g. a math proof. To prove something with a computer requires different computation machinery than mainstream program execution (e.g. a theorem prover). So we often substitute mathmatical rigour for pragmatic implementation and perform invariant testing by checking an invariant holds for lots of different configurations instead of proving that it holds for all.
  </script>
  <script id="273" type="text/markdown">
    The problem then becomes what configurations to check. The more configurations you check the slower invariant testing becomes as there are more cases to run. Some existing approaches involve randomized property checking, fuzz checking, AI etc.
  </script>
  <script id="282" type="text/markdown">
    For lopecode development tooling we often want to check invariants against code itself. The configuration space is therefore "all possible codable cells". It is quite difficult to generate random code cells that cover the configuration space sufficiently. So instead, we use the notebook runtime itself as the source for representative code cells.
  </script>
  <script id="293" type="text/markdown">
    ### Example: checking all runtime cells can be decompiled
  </script>
  <script id="296" type="text/markdown">
    Lopecode's source representation is `Observable JS`, which is almost Javascript but adds additional features like `viewof` and the ability to auto-detect inter-cell dependancies. For Lopecode to be compatible with Observable Notebooks it needs to be able to reverse the compilation process and generate Observable source code from the runtime graph expressed in Javascript -- i.e. _decompilation_. So it is important for correct Lopecode operation that every runtime graph can be decompiled back into Observable source
  </script>
  <script id="314" type="text/markdown">
    However, coming up with failing decompilation cases is _hard_. Particularly as Observable JS has no spec, and the implementor of Lopecode is not part of the organization that defined Observable JS. Instead of trying to think of all the cases, we leverage runtime reflection to check that all cell in the _current_ notebook are decompilable.

    ${test_all_cells_decompilable && ""}
  </script>
  <script id="356" type="application/vnd.observable.javascript">
    import { test_all_cells_decompilable } from "@tomlarkworthy/observablejs-toolchain"
  </script>
  <script id="433" type="application/vnd.observable.javascript">
    tests({
      filter: (test) =>
        test.name.includes("observablejs-toolchain#test_all_cells_decompilable") &&
        !test.name.startsWith("#")
    })
  </script>
  <script id="327" type="text/markdown">
    And so we write a single test, that checks for the invariant, that all runtime variables are decompilable, using the runtime graph itself as the source of interesting test cases. As this test is written with a `test_` prefix, it is automatically run when the notebook graph changes, and failures are highlighted immediately. By embedding this test in the decompiler source code, that all lopebooks contain, we actually end up running it over all source code that is ever used written in lopecode! 
  </script>
  <script id="353" type="text/markdown">
    ### Results

    On turning on the meta-tests I immediately found two bugs

    1. `await` inside functions failed
    2. Property definitions inside ObserverableJS `class` did not work due to missing feature in a 3rd party dependancy (`escodegen` [bug](https://github.com/estools/escodegen/issues/443) ) 

    Bug 2. is a particularly hard bug to spot, as it is in quite a specific part of of the JavaScript language. Both those issues have been converted into dedicated unit tests.
  </script>
  <script id="407" type="text/markdown">
    ## Choosing the right testing style
  </script>
  <script id="410" type="text/markdown">
    Invariant testing is very powerful at finding hard to find bugs, but then its often quite difficult to extract the individual situation that caused the failure. Unit tests are great for asserting specific features and situations work correctly. When a unit tests breaks, its usually obvious which code path is faulty.

    Invariant testing has found bugs I was unable to write unit tests for. On discovery of these bugs, I crystallise the knowledge by writing a unit test for them. I think the ideal testing structure is unit tests test every you know about, and invariant tests catch the things you do not know about, and over time everything invariant testing caught becomes dedicated unit tests. 
  </script>
  <script id="456" type="application/vnd.observable.javascript">
    architecture = FileAttachment("image@1.png")
  </script>
</notebook>
